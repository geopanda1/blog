[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Some tools to write better python code\n\n\n\n\n\n\n\npython\n\n\n\n\nSmall list of VSCode extensions and other things that help to improve code quality with limited effort\n\n\n\n\n\n\nMay 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction dispatch\n\n\n\n\n\n\n\npython\n\n\n\n\n3 ways to dispatch a function based on a string\n\n\n\n\n\n\nApr 29, 2025\n\n\n\n\n\n\n  \n\n\n\n\nHigh-resolution elevation data for Tyrol\n\n\n\n\n\n\n\npython-package\n\n\ndata-download\n\n\n\n\nPython package for downloading DEM tiles at 50 cm resolution for Tyrol\n\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n  \n\n\n\n\nLWD Tirol Meteo Download Package\n\n\n\n\n\n\n\npython-package\n\n\ndata-download\n\n\n\n\nPython package for downloading meteorological data of stations by the Avalanche Warning Service of Tyrol\n\n\n\n\n\n\nApr 8, 2025\n\n\n\n\n\n\n  \n\n\n\n\nGeosphere Data Download\n\n\n\n\n\n\n\npython-package\n\n\ndata-download\n\n\n\n\nPython package for downloading meteorological data of stations by Geosphere Austria\n\n\n\n\n\n\nApr 8, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website contains a small collection of code snippets I irregularily use and want to keep online as a reference for myself as well as some posts that link to some of the python packages I have developed at work."
  },
  {
    "objectID": "code-snippets.html",
    "href": "code-snippets.html",
    "title": "Code snippets",
    "section": "",
    "text": "Various pieces of code I more or less regularily use, as a reference for myself."
  },
  {
    "objectID": "code-snippets/data-download/00_download_s2_stac_asset.html",
    "href": "code-snippets/data-download/00_download_s2_stac_asset.html",
    "title": "stac-asset",
    "section": "",
    "text": "Download Sentinel-2 via command line using stac-asset\n\nrequires installation of the stac-asset library\n\nquickly download stac items via python interface or command line\n\n\nimport os\nfrom pathlib import Path\n\nfrom pystac_client import Client\nimport planetary_computer\n\nfrom bounding_box import lon_min, lat_min, lon_max, lat_max\n\n\ndata_dir = Path(\"./\")\nout_dir = data_dir / \"s2_data_stac-asset\"\nout_dir.mkdir()\n\n\n\nQuery some items\n\ncatalog = Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,  # not strictly necessary for finding the items!\n)\n\nquery = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[lon_min, lat_min, lon_max, lat_max],\n    datetime=\"2024-07-01/2024-07-30\",\n    query={\n        \"eo:cloud_cover\": {\"lt\": 20},\n    },\n)\n\nitems = list(query.items())\nprint(f\"Found: {len(items):d} datasets\")\n\nFound: 4 datasets\n\n\n\nlink_to_item = items[0].links[3].href\n\nprint(link_to_item)\n\nhttps://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a/items/S2A_MSIL2A_20240729T102021_R065_T32TPS_20240729T194952\n\n\n\n# resides in dedicated environment to avoid dependency conflicts\nstac_asset_path = \"/home/<user>/miniconda3/envs/stac/bin/stac-asset\"\n\n\ncmd = f\"{stac_asset_path} download {link_to_item} {out_dir}\"\n\n\nos.system(cmd)\n\n22/23: 100%|█████████▉| 1.74G/1.74G [00:50<00:00, 12.6MB/s, 0 errors]\n\n\n{\"type\": \"Feature\", \"stac_version\": \"1.0.0\", \"stac_extensions\": [\"https://stac-extensions.github.io/eo/v1.0.0/schema.json\", \"https://stac-extensions.github.io/sat/v1.0.0/schema.json\", \"https://stac-extensions.github.io/projection/v1.0.0/schema.json\"], \"id\": \"S2A_MSIL2A_20240729T102021_R065_T32TPS_20240729T194952\", \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[11.7520729, 46.8502664], [11.6924991, 46.7092583], [11.6277754, 46.564301], [11.5660321, 46.4186793], [11.5041944, 46.2732403], [11.4406416, 46.1284109], [11.3787445, 45.983333], [11.3613307, 45.9395307], [10.2904841, 45.9582645], [10.3140353, 46.9461683], [11.7555849, 46.9205364], [11.7520729, 46.8502664]]]}, \"bbox\": [10.2904841, 45.9395307, 11.7555849, 46.9461683], \"properties\": {\"datetime\": \"2024-07-29T10:20:21.025000Z\", \"platform\": \"Sentinel-2A\", \"proj:epsg\": 32632, \"instruments\": [\"msi\"], \"s2:mgrs_tile\": \"32TPS\", \"constellation\": \"Sentinel 2\", \"s2:granule_id\": \"S2A_OPER_MSI_L2A_TL_2APS_20240729T194952_A047538_T32TPS_N05.11\", \"eo:cloud_cover\": 16.663046, \"s2:datatake_id\": \"GS2A_20240729T102021_047538_N05.11\", \"s2:product_uri\": \"S2A_MSIL2A_20240729T102021_N0511_R065_T32TPS_20240729T194952.SAFE\", \"s2:datastrip_id\": \"S2A_OPER_MSI_L2A_DS_2APS_20240729T194952_S20240729T102559_N05.11\", \"s2:product_type\": \"S2MSI2A\", \"sat:orbit_state\": \"descending\", \"s2:datatake_type\": \"INS-NOBS\", \"s2:generation_time\": \"2024-07-29T19:49:52.000000Z\", \"sat:relative_orbit\": 65, \"s2:water_percentage\": 0.341176, \"s2:mean_solar_zenith\": 30.0395059052058, \"s2:mean_solar_azimuth\": 153.55374565552, \"s2:processing_baseline\": \"05.11\", \"s2:snow_ice_percentage\": 1.480092, \"s2:vegetation_percentage\": 63.926095, \"s2:thin_cirrus_percentage\": 0.036718, \"s2:cloud_shadow_percentage\": 1.713186, \"s2:nodata_pixel_percentage\": 11.29414, \"s2:unclassified_percentage\": 0.794875, \"s2:not_vegetated_percentage\": 12.102448, \"s2:degraded_msi_data_percentage\": 0.0256, \"s2:high_proba_clouds_percentage\": 9.40951, \"s2:reflectance_conversion_factor\": 0.969413241562976, \"s2:medium_proba_clouds_percentage\": 7.216818, \"s2:saturated_defective_pixel_percentage\": 0.0}, \"links\": [{\"rel\": \"collection\", \"href\": \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a\", \"type\": \"application/json\"}, {\"rel\": \"parent\", \"href\": \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a\", \"type\": \"application/json\"}, {\"rel\": \"root\", \"href\": \"https://planetarycomputer.microsoft.com/api/stac/v1/\", \"type\": \"application/json\"}, {\"rel\": \"license\", \"href\": \"https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice\"}, {\"rel\": \"preview\", \"href\": \"https://planetarycomputer.microsoft.com/api/data/v1/item/map?collection=sentinel-2-l2a&item=S2A_MSIL2A_20240729T102021_R065_T32TPS_20240729T194952\", \"type\": \"text/html\", \"title\": \"Map of item\"}, {\"rel\": \"derived_from\", \"href\": \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a/items/S2A_MSIL2A_20240729T102021_R065_T32TPS_20240729T194952\"}], \"assets\": {\"AOT\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_AOT_10m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Aerosol optical thickness (AOT)\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [10980, 10980], \"proj:transform\": [10.0, 0.0, 600000.0, 0.0, -10.0, 5200020.0], \"gsd\": 10.0, \"roles\": [\"data\"]}, \"B01\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B01_60m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 1 - Coastal aerosol - 60m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [1830, 1830], \"proj:transform\": [60.0, 0.0, 600000.0, 0.0, -60.0, 5200020.0], \"gsd\": 60.0, \"eo:bands\": [{\"name\": \"B01\", \"common_name\": \"coastal\", \"description\": \"Band 1 - Coastal aerosol\", \"center_wavelength\": 0.443, \"full_width_half_max\": 0.027}], \"roles\": [\"data\"]}, \"B02\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B02_10m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 2 - Blue - 10m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [10980, 10980], \"proj:transform\": [10.0, 0.0, 600000.0, 0.0, -10.0, 5200020.0], \"gsd\": 10.0, \"eo:bands\": [{\"name\": \"B02\", \"common_name\": \"blue\", \"description\": \"Band 2 - Blue\", \"center_wavelength\": 0.49, \"full_width_half_max\": 0.098}], \"roles\": [\"data\"]}, \"B03\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B03_10m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 3 - Green - 10m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [10980, 10980], \"proj:transform\": [10.0, 0.0, 600000.0, 0.0, -10.0, 5200020.0], \"gsd\": 10.0, \"eo:bands\": [{\"name\": \"B03\", \"common_name\": \"green\", \"description\": \"Band 3 - Green\", \"center_wavelength\": 0.56, \"full_width_half_max\": 0.045}], \"roles\": [\"data\"]}, \"B04\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B04_10m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 4 - Red - 10m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [10980, 10980], \"proj:transform\": [10.0, 0.0, 600000.0, 0.0, -10.0, 5200020.0], \"gsd\": 10.0, \"eo:bands\": [{\"name\": \"B04\", \"common_name\": \"red\", \"description\": \"Band 4 - Red\", \"center_wavelength\": 0.665, \"full_width_half_max\": 0.038}], \"roles\": [\"data\"]}, \"B05\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B05_20m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 5 - Vegetation red edge 1 - 20m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [5490, 5490], \"proj:transform\": [20.0, 0.0, 600000.0, 0.0, -20.0, 5200020.0], \"gsd\": 20.0, \"eo:bands\": [{\"name\": \"B05\", \"common_name\": \"rededge\", \"description\": \"Band 5 - Vegetation red edge 1\", \"center_wavelength\": 0.704, \"full_width_half_max\": 0.019}], \"roles\": [\"data\"]}, \"B06\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B06_20m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 6 - Vegetation red edge 2 - 20m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [5490, 5490], \"proj:transform\": [20.0, 0.0, 600000.0, 0.0, -20.0, 5200020.0], \"gsd\": 20.0, \"eo:bands\": [{\"name\": \"B06\", \"common_name\": \"rededge\", \"description\": \"Band 6 - Vegetation red edge 2\", \"center_wavelength\": 0.74, \"full_width_half_max\": 0.018}], \"roles\": [\"data\"]}, \"B07\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B07_20m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 7 - Vegetation red edge 3 - 20m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [5490, 5490], \"proj:transform\": [20.0, 0.0, 600000.0, 0.0, -20.0, 5200020.0], \"gsd\": 20.0, \"eo:bands\": [{\"name\": \"B07\", \"common_name\": \"rededge\", \"description\": \"Band 7 - Vegetation red edge 3\", \"center_wavelength\": 0.783, \"full_width_half_max\": 0.028}], \"roles\": [\"data\"]}, \"B08\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B08_10m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 8 - NIR - 10m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [10980, 10980], \"proj:transform\": [10.0, 0.0, 600000.0, 0.0, -10.0, 5200020.0], \"gsd\": 10.0, \"eo:bands\": [{\"name\": \"B08\", \"common_name\": \"nir\", \"description\": \"Band 8 - NIR\", \"center_wavelength\": 0.842, \"full_width_half_max\": 0.145}], \"roles\": [\"data\"]}, \"B09\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B09_60m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 9 - Water vapor - 60m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [1830, 1830], \"proj:transform\": [60.0, 0.0, 600000.0, 0.0, -60.0, 5200020.0], \"gsd\": 60.0, \"eo:bands\": [{\"name\": \"B09\", \"description\": \"Band 9 - Water vapor\", \"center_wavelength\": 0.945, \"full_width_half_max\": 0.026}], \"roles\": [\"data\"]}, \"B11\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B11_20m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 11 - SWIR (1.6) - 20m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [5490, 5490], \"proj:transform\": [20.0, 0.0, 600000.0, 0.0, -20.0, 5200020.0], \"gsd\": 20.0, \"eo:bands\": [{\"name\": \"B11\", \"common_name\": \"swir16\", \"description\": \"Band 11 - SWIR (1.6)\", \"center_wavelength\": 1.61, \"full_width_half_max\": 0.143}], \"roles\": [\"data\"]}, \"B12\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B12_20m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 12 - SWIR (2.2) - 20m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [5490, 5490], \"proj:transform\": [20.0, 0.0, 600000.0, 0.0, -20.0, 5200020.0], \"gsd\": 20.0, \"eo:bands\": [{\"name\": \"B12\", \"common_name\": \"swir22\", \"description\": \"Band 12 - SWIR (2.2)\", \"center_wavelength\": 2.19, \"full_width_half_max\": 0.242}], \"roles\": [\"data\"]}, \"B8A\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_B8A_20m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Band 8A - Vegetation red edge 4 - 20m\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [5490, 5490], \"proj:transform\": [20.0, 0.0, 600000.0, 0.0, -20.0, 5200020.0], \"gsd\": 20.0, \"eo:bands\": [{\"name\": \"B8A\", \"common_name\": \"rededge\", \"description\": \"Band 8A - Vegetation red edge 4\", \"center_wavelength\": 0.865, \"full_width_half_max\": 0.033}], \"roles\": [\"data\"]}, \"SCL\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_SCL_20m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Scene classfication map (SCL)\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [5490, 5490], \"proj:transform\": [20.0, 0.0, 600000.0, 0.0, -20.0, 5200020.0], \"gsd\": 20.0, \"roles\": [\"data\"]}, \"WVP\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_WVP_10m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"Water vapour (WVP)\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [10980, 10980], \"proj:transform\": [10.0, 0.0, 600000.0, 0.0, -10.0, 5200020.0], \"gsd\": 10.0, \"roles\": [\"data\"]}, \"visual\": {\"href\": \"s2_data_stac-asset/T32TPS_20240729T102021_TCI_10m.tif\", \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\", \"title\": \"True color image\", \"proj:bbox\": [600000.0, 5090220.0, 709800.0, 5200020.0], \"proj:shape\": [10980, 10980], \"proj:transform\": [10.0, 0.0, 600000.0, 0.0, -10.0, 5200020.0], \"gsd\": 10.0, \"eo:bands\": [{\"name\": \"B04\", \"common_name\": \"red\", \"description\": \"Band 4 - Red\", \"center_wavelength\": 0.665, \"full_width_half_max\": 0.038}, {\"name\": \"B03\", \"common_name\": \"green\", \"description\": \"Band 3 - Green\", \"center_wavelength\": 0.56, \"full_width_half_max\": 0.045}, {\"name\": \"B02\", \"common_name\": \"blue\", \"description\": \"Band 2 - Blue\", \"center_wavelength\": 0.49, \"full_width_half_max\": 0.098}], \"roles\": [\"data\"]}, \"safe-manifest\": {\"href\": \"s2_data_stac-asset/manifest.safe\", \"type\": \"application/xml\", \"title\": \"SAFE manifest\", \"roles\": [\"metadata\"]}, \"granule-metadata\": {\"href\": \"s2_data_stac-asset/MTD_TL.xml\", \"type\": \"application/xml\", \"title\": \"Granule metadata\", \"roles\": [\"metadata\"]}, \"inspire-metadata\": {\"href\": \"s2_data_stac-asset/INSPIRE.xml\", \"type\": \"application/xml\", \"title\": \"INSPIRE metadata\", \"roles\": [\"metadata\"]}, \"product-metadata\": {\"href\": \"s2_data_stac-asset/MTD_MSIL2A.xml\", \"type\": \"application/xml\", \"title\": \"Product metadata\", \"roles\": [\"metadata\"]}, \"datastrip-metadata\": {\"href\": \"s2_data_stac-asset/MTD_DS.xml\", \"type\": \"application/xml\", \"title\": \"Datastrip metadata\", \"roles\": [\"metadata\"]}, \"tilejson\": {\"href\": \"s2_data_stac-asset/tilejson.json\", \"type\": \"application/json\", \"title\": \"TileJSON with default rendering\", \"roles\": [\"tiles\"]}, \"rendered_preview\": {\"href\": \"s2_data_stac-asset/preview.png\", \"type\": \"image/png\", \"title\": \"Rendered preview\", \"rel\": \"preview\", \"roles\": [\"overview\"]}}, \"collection\": \"sentinel-2-l2a\"}\n\n\n                                                                     \n\n23/23: 100%|██████████| 1.74G/1.74G [00:51<00:00, 36.7MB/s, 0 errors]\n\n\n0"
  },
  {
    "objectID": "code-snippets/data-download/02_download_s2_stackstac.html",
    "href": "code-snippets/data-download/02_download_s2_stackstac.html",
    "title": "stackstac.stack",
    "section": "",
    "text": "Data will be written to a temporary directory to illustrate how to remove attributes, which is necessary if you want to write the object as a zarr file to disk.\n\nfrom pathlib import Path\nimport sys\nimport tempfile\n\nimport numpy as np\nimport pystac_client\nimport stackstac\n\nfrom bounding_box import *\n\n\nprint(sys.version)\nprint(pystac_client.__version__)\nprint(stackstac.__version__)\n\n3.13.1 | packaged by conda-forge | (main, Dec  5 2024, 21:23:54) [GCC 13.3.0]\n0.8.6\n0.5.1\n\n\n\ntmp_dir = tempfile.TemporaryDirectory()\nout_dir = Path(tmp_dir.name)\n\n\ncatalog = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1/\")\n\nquery = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[lon_min, lat_min, lon_max, lat_max],\n    datetime=\"2024-07-01\",\n)\n\nitems = list(query.items())\nprint(f\"Found: {len(items):d} datasets\")\n\nFound: 2 datasets\n\n\n\nds = stackstac.stack(\n    items,\n    assets=[\"red\", \"green\", \"blue\", \"nir08\"],\n    resolution=10,\n    xy_coords=\"center\",\n    epsg=32632\n)\n\n\nds = ds.sel(x=slice(x_min, x_max), y=slice(y_max, y_min))\n\n\nds = ds.isel(time=[0]) # just take first timestep to reduce amount of data\n\n\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'stackstac-7f303e927bf4afe6baade82cf0bd2b4e' (time: 1,\n                                                                band: 4,\n                                                                y: 6675, x: 5329)> Size: 1GB\ndask.array<getitem, shape=(1, 4, 6675, 5329), dtype=float64, chunksize=(1, 1, 1024, 1024), chunktype=numpy.ndarray>\nCoordinates: (12/52)\n  * time                                     (time) datetime64[ns] 8B 2024-07...\n    id                                       (time) <U24 96B 'S2B_32TPT_20240...\n  * band                                     (band) <U5 80B 'red' ... 'nir08'\n  * x                                        (x) float64 43kB 6.217e+05 ... 6...\n  * y                                        (y) float64 53kB 5.237e+06 ... 5...\n    instruments                              <U3 12B 'msi'\n    ...                                       ...\n    raster:bands                             (band) object 32B {'nodata': 0, ...\n    title                                    (band) <U21 336B 'Red (band 4) -...\n    common_name                              (band) <U5 80B 'red' ... 'nir08'\n    center_wavelength                        (band) float64 32B 0.665 ... 0.865\n    full_width_half_max                      (band) float64 32B 0.038 ... 0.033\n    epsg                                     int64 8B 32632\nAttributes:\n    spec:        RasterSpec(epsg=32632, bounds=(598150, 5087460, 713750, 5302...\n    crs:         epsg:32632\n    transform:   | 10.00, 0.00, 598150.00|\\n| 0.00,-10.00, 5302570.00|\\n| 0.0...\n    resolution:  10xarray.DataArray'stackstac-7f303e927bf4afe6baade82cf0bd2b4e'time: 1band: 4y: 6675x: 5329dask.array<chunksize=(1, 1, 639, 716), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         1.06 GiB \n                         8.00 MiB \n                    \n                    \n                    \n                         Shape \n                         (1, 4, 6675, 5329) \n                         (1, 1, 1024, 1024) \n                    \n                    \n                         Dask graph \n                         168 chunks in 5 graph layers \n                    \n                    \n                         Data type \n                         float64 numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  1\n  1\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  5329\n  6675\n  4\n\n        \n    \nCoordinates: (52)time(time)datetime64[ns]2024-07-01T10:17:48.675000array(['2024-07-01T10:17:48.675000000'], dtype='datetime64[ns]')id(time)<U24'S2B_32TPT_20240701_0_L2A'array(['S2B_32TPT_20240701_0_L2A'], dtype='<U24')band(band)<U5'red' 'green' 'blue' 'nir08'array(['red', 'green', 'blue', 'nir08'], dtype='<U5')x(x)float646.217e+05 6.217e+05 ... 6.75e+05array([621715., 621725., 621735., ..., 674975., 674985., 674995.],\n      shape=(5329,))y(y)float645.237e+06 5.237e+06 ... 5.171e+06array([5237275., 5237265., 5237255., ..., 5170555., 5170545., 5170535.],\n      shape=(6675,))instruments()<U3'msi'array('msi', dtype='<U3')s2:datatake_type()<U8'INS-NOBS'array('INS-NOBS', dtype='<U8')mgrs:latitude_band()<U1'T'array('T', dtype='<U1')view:sun_azimuth(time)float64147.6array([147.56725077])s2:unclassified_percentage(time)float640.005876array([0.005876])updated(time)<U24'2024-07-01T15:45:23.617Z'array(['2024-07-01T15:45:23.617Z'], dtype='<U24')s2:medium_proba_clouds_percentage(time)float6426.11array([26.107556])earthsearch:s3_path(time)<U79's3://sentinel-cogs/sentinel-s2-...array(['s3://sentinel-cogs/sentinel-s2-l2a-cogs/32/T/PT/2024/7/S2B_32TPT_20240701_0_L2A'],\n      dtype='<U79')s2:product_type()<U7'S2MSI2A'array('S2MSI2A', dtype='<U7')s2:datastrip_id()<U64'S2B_OPER_MSI_L2A_DS_2BPS_202407...array('S2B_OPER_MSI_L2A_DS_2BPS_20240701T125445_S20240701T101053_N05.10',\n      dtype='<U64')s2:degraded_msi_data_percentage(time)float640.0978array([0.0978])s2:processing_baseline()<U5'05.10'array('05.10', dtype='<U5')processing:software()object{'sentinel2-to-stac': '0.1.1'}array({'sentinel2-to-stac': '0.1.1'}, dtype=object)grid:code(time)<U10'MGRS-32TPT'array(['MGRS-32TPT'], dtype='<U10')mgrs:utm_zone()int6432array(32)s2:dark_features_percentage(time)float640.000479array([0.000479])s2:generation_time()<U27'2024-07-01T12:54:45.000000Z'array('2024-07-01T12:54:45.000000Z', dtype='<U27')s2:thin_cirrus_percentage(time)float642.272array([2.2715])proj:code()<U10'EPSG:32632'array('EPSG:32632', dtype='<U10')s2:high_proba_clouds_percentage(time)float6469.49array([69.491315])created(time)<U24'2024-07-01T15:45:23.617Z'array(['2024-07-01T15:45:23.617Z'], dtype='<U24')constellation()<U10'sentinel-2'array('sentinel-2', dtype='<U10')s2:datatake_id()<U34'GS2B_20240701T100559_038229_N05...array('GS2B_20240701T100559_038229_N05.10', dtype='<U34')s2:snow_ice_percentage(time)float640.01595array([0.015945])mgrs:grid_square(time)<U2'PT'array(['PT'], dtype='<U2')platform()<U11'sentinel-2b'array('sentinel-2b', dtype='<U11')s2:granule_id(time)<U62'S2B_OPER_MSI_L2A_TL_2BPS_202407...array(['S2B_OPER_MSI_L2A_TL_2BPS_20240701T125445_A038229_T32TPT_N05.10'],\n      dtype='<U62')earthsearch:payload_id(time)<U74'roda-sentinel2/workflow-sentine...array(['roda-sentinel2/workflow-sentinel2-to-stac/cd2c9a0c8c33cff594b43f495a19f35e'],\n      dtype='<U74')s2:sequence()<U1'0'array('0', dtype='<U1')s2:nodata_pixel_percentage(time)object8.589324array([8.589324], dtype=object)s2:vegetation_percentage(time)float640.08036array([0.080359])s2:saturated_defective_pixel_percentage()int640array(0)s2:product_uri(time)<U65'S2B_MSIL2A_20240701T100559_N051...array(['S2B_MSIL2A_20240701T100559_N0510_R022_T32TPT_20240701T125445.SAFE'],\n      dtype='<U65')eo:cloud_cover(time)float6497.87array([97.870368])s2:cloud_shadow_percentage(time)float642.004array([2.003527])s2:not_vegetated_percentage(time)float640.01881array([0.018809])earthsearch:boa_offset_applied()boolTruearray(True)view:sun_elevation(time)float6462.73array([62.72922262])s2:water_percentage(time)float640.004639array([0.004639])s2:reflectance_conversion_factor()float640.9676array(0.9675944)gsd(band)int6410 10 10 20array([10, 10, 10, 20])raster:bands(band)object{'nodata': 0, 'data_type': 'uint...array([{'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 10, 'scale': 0.0001, 'offset': -0.1},\n       {'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 10, 'scale': 0.0001, 'offset': -0.1},\n       {'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 10, 'scale': 0.0001, 'offset': -0.1},\n       {'nodata': 0, 'data_type': 'uint16', 'bits_per_sample': 15, 'spatial_resolution': 20, 'scale': 0.0001, 'offset': -0.1}],\n      dtype=object)title(band)<U21'Red (band 4) - 10m' ... 'NIR 2 ...array(['Red (band 4) - 10m', 'Green (band 3) - 10m',\n       'Blue (band 2) - 10m', 'NIR 2 (band 8A) - 20m'], dtype='<U21')common_name(band)<U5'red' 'green' 'blue' 'nir08'array(['red', 'green', 'blue', 'nir08'], dtype='<U5')center_wavelength(band)float640.665 0.56 0.49 0.865array([0.665, 0.56 , 0.49 , 0.865])full_width_half_max(band)float640.038 0.045 0.098 0.033array([0.038, 0.045, 0.098, 0.033])epsg()int6432632array(32632)Indexes: (4)timePandasIndexPandasIndex(DatetimeIndex(['2024-07-01 10:17:48.675000'], dtype='datetime64[ns]', name='time', freq=None))bandPandasIndexPandasIndex(Index(['red', 'green', 'blue', 'nir08'], dtype='object', name='band'))xPandasIndexPandasIndex(Index([621715.0, 621725.0, 621735.0, 621745.0, 621755.0, 621765.0, 621775.0,\n       621785.0, 621795.0, 621805.0,\n       ...\n       674905.0, 674915.0, 674925.0, 674935.0, 674945.0, 674955.0, 674965.0,\n       674975.0, 674985.0, 674995.0],\n      dtype='float64', name='x', length=5329))yPandasIndexPandasIndex(Index([5237275.0, 5237265.0, 5237255.0, 5237245.0, 5237235.0, 5237225.0,\n       5237215.0, 5237205.0, 5237195.0, 5237185.0,\n       ...\n       5170625.0, 5170615.0, 5170605.0, 5170595.0, 5170585.0, 5170575.0,\n       5170565.0, 5170555.0, 5170545.0, 5170535.0],\n      dtype='float64', name='y', length=6675))Attributes: (4)spec :RasterSpec(epsg=32632, bounds=(598150, 5087460, 713750, 5302570), resolutions_xy=(10, 10))crs :epsg:32632transform :| 10.00, 0.00, 598150.00|\n| 0.00,-10.00, 5302570.00|\n| 0.00, 0.00, 1.00|resolution :10"
  },
  {
    "objectID": "code-snippets/data-download/02_download_s2_stackstac.html#remove-or-convert-coordinates-and-attributes-with-dtype-object",
    "href": "code-snippets/data-download/02_download_s2_stackstac.html#remove-or-convert-coordinates-and-attributes-with-dtype-object",
    "title": "stackstac.stack",
    "section": "Remove or convert coordinates and attributes with dtype object",
    "text": "Remove or convert coordinates and attributes with dtype object\nThese can’t be serialized by zarr\n\nds[\"s2:nodata_pixel_percentage\"] = ds[\"s2:nodata_pixel_percentage\"].astype(np.float32)\n\n\nfor coord in ds.coords:\n    if type(ds[coord].dtype) == np.dtypes.ObjectDType:\n        print(coord)\n\nprocessing:software\nraster:bands\n\n\n\nds = ds.drop_vars([\"raster:bands\", \"processing:software\"])"
  },
  {
    "objectID": "code-snippets/data-download/02_download_s2_stackstac.html#unify-chunks",
    "href": "code-snippets/data-download/02_download_s2_stackstac.html#unify-chunks",
    "title": "stackstac.stack",
    "section": "Unify chunks",
    "text": "Unify chunks\n\nds = ds.chunk(chunks={\"time\": 1, \"x\": 1000, \"y\": 1000})"
  },
  {
    "objectID": "code-snippets/data-download/02_download_s2_stackstac.html#remove-attributes---be-careful-geotransform-probably-lost",
    "href": "code-snippets/data-download/02_download_s2_stackstac.html#remove-attributes---be-careful-geotransform-probably-lost",
    "title": "stackstac.stack",
    "section": "Remove attributes - be careful, geotransform probably lost",
    "text": "Remove attributes - be careful, geotransform probably lost\n\nds.attrs\n\n{'spec': RasterSpec(epsg=32632, bounds=(598150, 5087460, 713750, 5302570), resolutions_xy=(10, 10)),\n 'crs': 'epsg:32632',\n 'transform': Affine(10.0, 0.0, 598150.0,\n        0.0, -10.0, 5302570.0),\n 'resolution': 10}\n\n\n\nds.attrs = {}\n\n\nds.to_zarr(out_dir / f\"s2_{ds.time.dt.strftime('%Y-%m-%d').values[0]}.zarr\")"
  },
  {
    "objectID": "code-snippets/data-download/02_download_s2_stackstac.html#clean-up",
    "href": "code-snippets/data-download/02_download_s2_stackstac.html#clean-up",
    "title": "stackstac.stack",
    "section": "Clean up",
    "text": "Clean up\n\ntmp_dir.cleanup()"
  },
  {
    "objectID": "code-snippets/data-download/01_download_s2_odc-stac.html",
    "href": "code-snippets/data-download/01_download_s2_odc-stac.html",
    "title": "odc.stack.stac_load",
    "section": "",
    "text": "Query Sentinel-2 data via odc.stac.stac_load into xarray.Dataset\n\nfrom pystac_client import Client\nimport planetary_computer\nfrom odc.stac import stac_load\n\nfrom bounding_box import *\n\n\n\n\n\n\n\nImportant\n\n\n\nSet modifier in client. Otherwise only the search will succeed, but data access will fail\n\n\n\ncatalog = Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\nquery = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[lon_min, lat_min, lon_max, lat_max],\n    datetime=\"2024-07-01\",\n)\n\nitems = list(query.items())\nprint(f\"Found: {len(items):d} datasets\")\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember to set chunks, otherwise the data will be loaded eagerly!\n\n\n\nchunk_size = 1024\n\nds = stac_load(\n    items,\n    bands=(\"B04\", \"B03\", \"B02\", \"B08\"),\n    resolution=10,\n    chunks={\"time\": 1, \"x\": chunk_size, \"y\": chunk_size} # this prevents eager loading of data\n)\nds\n\n\nds = ds.sel(x=slice(x_min, x_max), y=slice(y_max, y_min))\n\n\nds = ds.isel(time=[0]) # just take first timestep to reduce amount of data\n\n\n# load data into memory if desired\nds.load()\n\n\nds[\"B04\"].squeeze().plot.imshow()\n\n… write to disk, if necessary …"
  },
  {
    "objectID": "code-snippets/pandas/00_groupyby-agg.html",
    "href": "code-snippets/pandas/00_groupyby-agg.html",
    "title": "groupby(…).agg(…)",
    "section": "",
    "text": "How to use built-in or custom functions in groupby operations and set the name of the resulting column.\n\nimport pandas as pd\nimport numpy as np\n\n# use iris dataset from seaborn package\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n)\n\ndf_grouped = (\n    df.groupby(\"species\")\n    .agg(\n        pl_min=(\"petal_length\", \"min\"),\n        pl_max=(\"petal_length\", lambda x: max(x)),\n        pl_perc95=(\"petal_length\", lambda x: np.percentile(x, 95)),\n    )\n    .reset_index()\n)\n\ndf_grouped\n\n\n\n\n\n  \n    \n      \n      species\n      pl_min\n      pl_max\n      pl_perc95\n    \n  \n  \n    \n      0\n      setosa\n      1.0\n      1.9\n      1.700\n    \n    \n      1\n      versicolor\n      3.0\n      5.1\n      4.900\n    \n    \n      2\n      virginica\n      4.5\n      6.9\n      6.655\n    \n  \n\n\n\n\n\nAggregate all rows\n\nimport pandas as pd\nimport numpy as np\n\n# use iris dataset from seaborn package\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n)\n\ndf_grouped = (\n    df.groupby(lambda x: True)  # from https://stackoverflow.com/a/46583472\n    .agg(\n        pl_min=(\"petal_length\", \"min\"),\n        pl_max=(\"petal_length\", lambda x: max(x)),\n        pl_perc95=(\"petal_length\", lambda x: np.percentile(x, 95)),\n    )\n    .reset_index()\n)\n\ndf_grouped\n\n\n\n\n\n  \n    \n      \n      index\n      pl_min\n      pl_max\n      pl_perc95\n    \n  \n  \n    \n      0\n      True\n      1.0\n      6.9\n      6.1"
  },
  {
    "objectID": "code-snippets/requests/00_get_request.html",
    "href": "code-snippets/requests/00_get_request.html",
    "title": "requests module",
    "section": "",
    "text": "import requests\n\nurl = \"....\"\n\nr = requests.get(url)\n\n# retrieve status code\nr.status_code \n\n# retrieve json formatted response\nr.json()\n\n# write non-binary output\nwith (\"out-file.txt\", \"w\") as fobj:\n    fobj.write(r.content)\n\n\n# write binary output (e.g., for zip-files and other binary formats)\nwith (\"out-file\", \"wb\") as fobj:\n    fobj.write(r.content)\nRecursive request in case of non-successful return code:\n\nurl = \"....\"\n\ndef request_until_success(url):\n    r = requests.get(url)\n\n    if r. status_code != 200:\n        # possibly delay renewed requests here ...\n\n        return request_until_success(url)\n\n    return r"
  },
  {
    "objectID": "code-snippets/linux/apt.html",
    "href": "code-snippets/linux/apt.html",
    "title": "apt",
    "section": "",
    "text": "Differences between the older apt-get and apt are detailed in this amazon article\n# update/upgrade\napt update\napt upgrade\n\n# search package \napt search <search-string>\n\n# install\napt install <package>\n\n# add ppa\nadd-apt-repository <ppa:repo>\n\n# remove ppa - sometimes old ppas don't have release candidates\nadd-apt-repository --remove <ppa:repo>"
  },
  {
    "objectID": "code-snippets/dask/00_local_cluster.html",
    "href": "code-snippets/dask/00_local_cluster.html",
    "title": "Local cluster",
    "section": "",
    "text": "import multiprocessing\nfrom dask.distributed import Client, LocalCluster\n\n\nSetup up local dask cluster\n\npossibly adjust number of threads per worker\ndon’t forget to put the Client(...) in a if __name__ == \"__main__\" context when running from a script\n\n\nn_workers = multiprocessing.cpu_count()\n\nmem_buffer = 10 # how much memory will be spared from workers\n\ngb_total = 128 # total memory of machine\ngb_available = gb_total - mem_buffer # what is left for dask\ngb_per_worker = int(gb_total / n_workers) # memory for each dask worker\n\n\nclient = Client(\n    address=LocalCluster(\n        n_workers=n_workers,\n        threads_per_worker=2,\n        interface=\"lo\",\n        memory_limit=f\"{gb_per_worker}GB\",\n    )\n)\n\nInspect link to view dashboard\n\nprint(client.dashboard_link)\n\nhttp://127.0.0.1:8787/status"
  },
  {
    "objectID": "code-snippets/joblib/00_parallel_everything.html",
    "href": "code-snippets/joblib/00_parallel_everything.html",
    "title": "Multithreading and -processing with joblib",
    "section": "",
    "text": "from joblib import Parallel, delayed\n\nCreate a function to be executed in parallel:\n\ndef my_embarassingly_parallel_job(arg):\n    # perform work...\n    return\n\nUses the default “loky” backend for process based parallelism:\n\nresults = Parallel(n_jobs=2)(\n    delayed(my_embarassingly_parallel_job)(i) for i in range(10)\n)\n\nThread-based parallelism:\n\nresults = Parallel(n_jobs=2, prefer=\"threads\")(\n    delayed(my_embarassingly_parallel_job)(i) for i in range(10)\n)"
  },
  {
    "objectID": "code-snippets/sqlalchemy/00_relations.html",
    "href": "code-snippets/sqlalchemy/00_relations.html",
    "title": "Relations",
    "section": "",
    "text": "How do define relationships without explicitly setting foreign keys.\n\nimport sqlalchemy as db\n\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.orm import declarative_base, relationship, Session\nBase = declarative_base()\n\n\nclass User(Base):\n    __tablename__ = \"user\"\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String)\n    products = relationship(\"Product\", back_populates=\"user\")\n\nclass Product(Base):\n    __tablename__ = \"product\"\n    id = db.Column(db.Integer, primary_key=True)\n    name = db.Column(db.String)\n    user_id = db.Column(db.Integer, db.ForeignKey(\"user.id\"), nullable=False)\n    user = relationship(\"User\", back_populates=\"products\")\n\n\ndb_path = \"sqlite:///test.db\"\nengine = db.create_engine(db_path)\nBase.metadata.create_all(engine, checkfirst=True)\n\nsession = Session(bind=engine)\n# enforce foreign key constraint in SQLite (off by default!)\nsession.execute(text(\"PRAGMA foreign_keys=on\"));\n\n\nuser = User(name=\"Alice\")\nproduct1 = Product(name=\"A\", user=user)\nproduct2 = Product(name=\"B\", user=user)\n\nsession.add(user)\nsession.commit()\n\nForeign key will be correctly assigned by sqlalchemy:\n\nproduct1.user_id\n\n1\n\n\nWorks also the other way round:\n\nuser = User(name=\"Kyle\")\nproduct3 = Product(name=\"A\")\nproduct4 = Product(name=\"B\")\n\nuser.products = [product3, product4]\n\nsession.add(user)\nsession.commit()\n\nForeign key will be correctly assigned by sqlalchemy:\n\nproduct3.user_id\n\n2\n\n\n\nThis does not work\nThe associated user.id will not be set correctly. It will be set to None which is the current value of user.id prior to committing.\n\nuser = User(name=\"Dave\")\n\nproduct5 = Product(name=\"C\", user_id=user.id) # user.id is None at this point!\nproduct6 = Product(name=\"D\", user_id=user.id)\n\nsession.add_all([user, product5, product6])\nsession.commit()\n\nIntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: product.user_id\n[SQL: INSERT INTO product (name, user_id) VALUES (?, ?) RETURNING id]\n[parameters: ('C', None)]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)"
  },
  {
    "objectID": "code-snippets/sqlalchemy/01_views.html",
    "href": "code-snippets/sqlalchemy/01_views.html",
    "title": "View with model",
    "section": "",
    "text": "How do create a view with a proper model. tbc"
  },
  {
    "objectID": "code-snippets/xarray/01_rf_predict_apply_ufunc.html",
    "href": "code-snippets/xarray/01_rf_predict_apply_ufunc.html",
    "title": "xr.apply_ufunc(…)",
    "section": "",
    "text": "Example on how xr.apply_ufunc(...) can be used for pixel wise prediction."
  },
  {
    "objectID": "code-snippets/xarray/01_rf_predict_apply_ufunc.html#generate-some-random-data-for-training-and-inference",
    "href": "code-snippets/xarray/01_rf_predict_apply_ufunc.html#generate-some-random-data-for-training-and-inference",
    "title": "xr.apply_ufunc(…)",
    "section": "Generate some random data for training and inference",
    "text": "Generate some random data for training and inference\n\n\nCode\nn_classes = 2\nn_features = 12\nn_samples = 1000\n\nlat = 40\nlon = 60\ntime = n_features\n\n\n\n# random training data\nX_train, y_train = generate_X_y(n_samples,n_features, n_classes)\n\n\n# random \"real\" data to predict on\nds = generate_3d_dataset(lat, lon, time)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset> Size: 231kB\nDimensions:  (lat: 40, lon: 60, time: 12)\nCoordinates:\n  * lat      (lat) int64 320B 0 1 2 3 4 5 6 7 8 9 ... 31 32 33 34 35 36 37 38 39\n  * lon      (lon) int64 480B 0 1 2 3 4 5 6 7 8 9 ... 51 52 53 54 55 56 57 58 59\n  * time     (time) datetime64[ns] 96B 2021-01-01 2021-01-02 ... 2021-01-12\nData variables:\n    test     (lat, lon, time) float64 230kB dask.array<chunksize=(4, 6, 12), meta=np.ndarray>xarray.DatasetDimensions:lat: 40lon: 60time: 12Coordinates: (3)lat(lat)int640 1 2 3 4 5 6 ... 34 35 36 37 38 39array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39])lon(lon)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])time(time)datetime64[ns]2021-01-01 ... 2021-01-12array(['2021-01-01T00:00:00.000000000', '2021-01-02T00:00:00.000000000',\n       '2021-01-03T00:00:00.000000000', '2021-01-04T00:00:00.000000000',\n       '2021-01-05T00:00:00.000000000', '2021-01-06T00:00:00.000000000',\n       '2021-01-07T00:00:00.000000000', '2021-01-08T00:00:00.000000000',\n       '2021-01-09T00:00:00.000000000', '2021-01-10T00:00:00.000000000',\n       '2021-01-11T00:00:00.000000000', '2021-01-12T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)test(lat, lon, time)float64dask.array<chunksize=(4, 6, 12), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         225.00 kiB \n                         2.25 kiB \n                    \n                    \n                    \n                         Shape \n                         (40, 60, 12) \n                         (4, 6, 12) \n                    \n                    \n                         Dask graph \n                         100 chunks in 1 graph layer \n                    \n                    \n                         Data type \n                         float64 numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  12\n  60\n  40\n\n        \n    \nIndexes: (3)latPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39],\n      dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04',\n               '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08',\n               '2021-01-09', '2021-01-10', '2021-01-11', '2021-01-12'],\n              dtype='datetime64[ns]', name='time', freq='D'))Attributes: (0)"
  },
  {
    "objectID": "code-snippets/xarray/01_rf_predict_apply_ufunc.html#train-a-dummy-model",
    "href": "code-snippets/xarray/01_rf_predict_apply_ufunc.html#train-a-dummy-model",
    "title": "xr.apply_ufunc(…)",
    "section": "Train a dummy model",
    "text": "Train a dummy model\n\nrf = RF(random_state=42, n_estimators=50, n_jobs=-1)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)"
  },
  {
    "objectID": "code-snippets/xarray/01_rf_predict_apply_ufunc.html#define-function-to-be-applied-via-.apply.ufunc...",
    "href": "code-snippets/xarray/01_rf_predict_apply_ufunc.html#define-function-to-be-applied-via-.apply.ufunc...",
    "title": "xr.apply_ufunc(…)",
    "section": "Define function to be applied via .apply.ufunc(...)",
    "text": "Define function to be applied via .apply.ufunc(...)\n\ndef generic_func(arr):\n    return rf.predict(arr.reshape(1, -1))\n\n\nds_ag = xr.apply_ufunc(\n    generic_func,\n    ds,\n    input_core_dims=[[\"time\"]],\n    dask=\"parallelized\",\n    output_dtypes=np.float32,\n    vectorize=True,\n    dask_gufunc_kwargs={\"allow_rechunk\": True},\n)\n\n\nds_ag\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset> Size: 10kB\nDimensions:  (lat: 40, lon: 60)\nCoordinates:\n  * lat      (lat) int64 320B 0 1 2 3 4 5 6 7 8 9 ... 31 32 33 34 35 36 37 38 39\n  * lon      (lon) int64 480B 0 1 2 3 4 5 6 7 8 9 ... 51 52 53 54 55 56 57 58 59\nData variables:\n    test     (lat, lon) float32 10kB dask.array<chunksize=(4, 6), meta=np.ndarray>xarray.DatasetDimensions:lat: 40lon: 60Coordinates: (2)lat(lat)int640 1 2 3 4 5 6 ... 34 35 36 37 38 39array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39])lon(lon)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])Data variables: (1)test(lat, lon)float32dask.array<chunksize=(4, 6), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         9.38 kiB \n                         96 B \n                    \n                    \n                    \n                         Shape \n                         (40, 60) \n                         (4, 6) \n                    \n                    \n                         Dask graph \n                         100 chunks in 4 graph layers \n                    \n                    \n                         Data type \n                         float32 numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  60\n  40\n\n        \n    \nIndexes: (2)latPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39],\n      dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='lon'))Attributes: (0)\n\n\n\nds_ag.compute()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset> Size: 10kB\nDimensions:  (lat: 40, lon: 60)\nCoordinates:\n  * lat      (lat) int64 320B 0 1 2 3 4 5 6 7 8 9 ... 31 32 33 34 35 36 37 38 39\n  * lon      (lon) int64 480B 0 1 2 3 4 5 6 7 8 9 ... 51 52 53 54 55 56 57 58 59\nData variables:\n    test     (lat, lon) float32 10kB 0.0 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 0.0 0.0xarray.DatasetDimensions:lat: 40lon: 60Coordinates: (2)lat(lat)int640 1 2 3 4 5 6 ... 34 35 36 37 38 39array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39])lon(lon)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])Data variables: (1)test(lat, lon)float320.0 1.0 1.0 1.0 ... 1.0 1.0 0.0 0.0array([[0., 1., 1., ..., 1., 1., 0.],\n       [1., 1., 0., ..., 0., 1., 0.],\n       [0., 0., 1., ..., 1., 0., 1.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 1.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 1., 0., 0.]], shape=(40, 60), dtype=float32)Indexes: (2)latPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39],\n      dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='lon'))Attributes: (0)"
  },
  {
    "objectID": "code-snippets/xarray/04_geotiff-to-xarray.html",
    "href": "code-snippets/xarray/04_geotiff-to-xarray.html",
    "title": "Geotiff-to-xarray",
    "section": "",
    "text": "Generate a xarray.Dataset from a list of geotiffs.\n\nfrom pathlib import Path\nimport datetime\n\nimport xarray as xr\nimport rioxarray\n\n\nprint(xr.__version__)\nprint(rioxarray.__version__)\n\n2025.3.1\n0.18.2\n\n\n\ngeotiff_dir = Path(\"...\")\n\n\ndef get_date(filename: Path) -> datetime.datetime:\n    \"\"\"Parse date from filename - adapt to your needs\"\"\"\n    f = filename.stem\n    date_str = f.split(\"_\")[-1]\n    return datetime.datetime.strptime(date_str, \"%Y-%m-%d\")\n\n\nGet list of geotiffs\n\ngeotiffs = list(geotiff_dir.glob(\"*.tif\"))\ngeotiffs.sort(key=get_date)\n\n\n\nPerform the actual read\nImportant: The geotiffs themselves have to be tiled to enable chunked loading/processing (see this github issue), just setting chunks={...} in .open_rasterio is not sufficient.\n\nda_s = [\n    rioxarray.open_rasterio(\n        raster,\n        default_name=\"snow_cover\",\n        chunks={\"band\": 1, \"x\": 1024, \"y\": 1024},\n    ).squeeze(dim=\"band\", drop=True)\n    for raster in geotiffs\n]\n\n\ntime_var = xr.Variable(\"time\", [get_date(img) for img in geotiffs])\n\n\n\nConcatenate aling time dimension\n\nds = xr.concat(da_s, dim=time_var).to_dataset()\n\n\n# write source directory into attributes to preserve the origin\nds.attrs[\"base_dir\"] = geotiff_dir.as_posix()\n\n\n\nWrite to disk as zarr\n\nds.to_zarr(\"data.zarr\")"
  },
  {
    "objectID": "code-snippets/xarray/03_resampling_xr_reproject.html",
    "href": "code-snippets/xarray/03_resampling_xr_reproject.html",
    "title": "Lazy resampling",
    "section": "",
    "text": "Example on how to lazily resample a xarray.Dataset or xarray.DataArray with dask using odc.geo.xr.xr_reproject\n\nimport sys\nimport numpy as np\nimport xarray as xr\nfrom rasterio.enums import Resampling\nimport odc.geo\nfrom odc.geo.xr import xr_reproject\nfrom odc.geo.geobox import GeoBox, BoundingBox\n\n\nprint(sys.version)\nprint(xr.__version__)\nprint(odc.geo.__version__)\n\n3.13.1 | packaged by conda-forge | (main, Dec  5 2024, 21:23:54) [GCC 13.3.0]\n2025.3.1\n0.4.10\n\n\nThe following function is only to simluate that the crs, while initially present on the opened dataset, can get lost easily along the way in workflows, leading to the reprojection to fail. This is not a shortcoming of xarray or xr_reproject, but I want to document it, as this happened to me several times before.\n\ndef replace_2_by_0(da: xr.DataArray) -> xr.DataArray:\n    \"\"\"Replace 2s by 0\"\"\"\n    return da.where(da != 2, 0)\n\n\nds_webcam = xr.open_zarr(\"./data/webcam_snow_cover.zarr\")\n\n\nds_webcam[\"nodata\"] = np.nan\n\n\nCheck that important info on crs is present\n\nassert ds_webcam.odc.geobox\n\n\nassert ds_webcam.odc.crs\n\n\n\nConstruct geobox needed for xr_reproject\n\nxmin, xmax = 656320.0, 661020.0\nymax, ymin = 5213480.0, 5209240.0\n\nlower_res = 20 # target resolution in meters\n\nepsg_code = 25832\n\n\nbbox = BoundingBox(left=xmin, top=ymax, bottom=ymin, right=xmax)\ngeobox = GeoBox.from_bbox(bbox=bbox, crs=epsg_code, resolution=lower_res)\n\n\n\nPerform computation on dataset\nI sometimes use xr.DataArray objects in computations. It is easy to oversee that the data array (here ds_webcam.snow_cover) does not inherit or carry over all the important information (.odc.crs and .odc.geobox) present on the parent dataset object (ds_webcam) when used in computations.\n\n# perform the dummy operation\nsnow_cover = replace_2_by_0(ds_webcam.snow_cover)\n\n\n# geobox is present!\nassert snow_cover.odc.geobox\n\n/home/<user>/miniconda3/envs/satpy/lib/python3.13/site-packages/odc/geo/_xr_interop.py:503: UserWarning: grid_mapping=spatial_ref is not pointing to valid coordinate\n  warnings.warn(\n\n\n\n# but the crs is not (as the above warning indicates)\nsnow_cover.odc.crs\n\nTo me this was initially counter intuitive, as the parent dataset has the complete information:\n\nds_webcam.odc.crs\n\nCRS('PROJCS[\"ETRS89 / UTM zone 32N\",GEOGCS[\"ETRS89\",DATUM[\"European_Terrestrial_Reference_System_1989\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6258\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4258\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",9],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"25832\"]]')\n\n\nLet’s fix that:\n\nsnow_cover = snow_cover.odc.assign_crs(\"epsg:25832\")\n\n\n\nCheck source no data value and set correctly\n\n# show that nodata attribute is missing\nassert getattr(snow_cover, \"nodata\", None) is None\n\n\n# add nodata attribute\nsnow_cover[\"nodata\"] = np.nan\n\n\n\nPerform lazy resampling\n\nsnow_cover_downsampled = xr_reproject(snow_cover, geobox, resampling=Resampling.mode)\nsnow_cover_downsampled.name = \"snow_cover\"\n\nWhich returns a dask-backed dataarray:\n\nsnow_cover_downsampled\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.DataArray 'snow_cover' (time: 1, y: 212, x: 235)> Size: 399kB\ndask.array<reproject, shape=(1, 212, 235), dtype=float64, chunksize=(1, 212, 235), chunktype=numpy.ndarray>\nCoordinates:\n  * time         (time) datetime64[ns] 8B 2021-07-15\n    nodata       float64 8B nan\n  * y            (y) float64 2kB 5.213e+06 5.213e+06 ... 5.209e+06 5.209e+06\n  * x            (x) float64 2kB 6.563e+05 6.564e+05 ... 6.61e+05 6.61e+05\n    spatial_ref  int32 4B 25832\nAttributes:\n    AREA_OR_POINT:  Areaxarray.DataArray'snow_cover'time: 1y: 212x: 235dask.array<chunksize=(1, 212, 235), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         389.22 kiB \n                         389.22 kiB \n                    \n                    \n                    \n                         Shape \n                         (1, 212, 235) \n                         (1, 212, 235) \n                    \n                    \n                         Dask graph \n                         1 chunks in 5 graph layers \n                    \n                    \n                         Data type \n                         float64 numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  235\n  212\n  1\n\n        \n    \nCoordinates: (5)time(time)datetime64[ns]2021-07-15array(['2021-07-15T00:00:00.000000000'], dtype='datetime64[ns]')nodata()float64nanarray(nan)y(y)float645.213e+06 5.213e+06 ... 5.209e+06units :metreresolution :-20.0crs :EPSG:25832array([5213470., 5213450., 5213430., ..., 5209290., 5209270., 5209250.],\n      shape=(212,))x(x)float646.563e+05 6.564e+05 ... 6.61e+05units :metreresolution :20.0crs :EPSG:25832array([656330., 656350., 656370., ..., 660970., 660990., 661010.], shape=(235,))spatial_ref()int3225832spatial_ref :PROJCRS[\"ETRS89 / UTM zone 32N\",BASEGEOGCRS[\"ETRS89\",DATUM[\"European Terrestrial Reference System 1989\",ELLIPSOID[\"GRS 1980\",6378137,298.257222101,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4258]],CONVERSION[\"UTM zone 32N\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",9,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",500000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"Europe between 6°E and 12°E: Austria; Denmark - onshore and offshore; Germany - onshore and offshore; Italy - onshore and offshore; Norway including Svalbard - onshore and offshore; Spain - offshore.\"],BBOX[36.53,6,84.01,12.01]],USAGE[SCOPE[\"Pan-European conformal mapping at scales larger than 1:500,000.\"],AREA[\"Europe between 6°E and 12°E and approximately 36°30'N to 84°N.\"],BBOX[36.53,6,84.01,12.01]],ID[\"EPSG\",25832]]crs_wkt :PROJCRS[\"ETRS89 / UTM zone 32N\",BASEGEOGCRS[\"ETRS89\",DATUM[\"European Terrestrial Reference System 1989\",ELLIPSOID[\"GRS 1980\",6378137,298.257222101,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4258]],CONVERSION[\"UTM zone 32N\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",9,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",500000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",0,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"Europe between 6°E and 12°E: Austria; Denmark - onshore and offshore; Germany - onshore and offshore; Italy - onshore and offshore; Norway including Svalbard - onshore and offshore; Spain - offshore.\"],BBOX[36.53,6,84.01,12.01]],USAGE[SCOPE[\"Pan-European conformal mapping at scales larger than 1:500,000.\"],AREA[\"Europe between 6°E and 12°E and approximately 36°30'N to 84°N.\"],BBOX[36.53,6,84.01,12.01]],ID[\"EPSG\",25832]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :ETRS89horizontal_datum_name :European Terrestrial Reference System 1989projected_crs_name :ETRS89 / UTM zone 32Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :9.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996GeoTransform :656320 20 0 5213480 0 -20array(25832, dtype=int32)Indexes: (3)timePandasIndexPandasIndex(DatetimeIndex(['2021-07-15'], dtype='datetime64[ns]', name='time', freq=None))yPandasIndexPandasIndex(Index([5213470.0, 5213450.0, 5213430.0, 5213410.0, 5213390.0, 5213370.0,\n       5213350.0, 5213330.0, 5213310.0, 5213290.0,\n       ...\n       5209430.0, 5209410.0, 5209390.0, 5209370.0, 5209350.0, 5209330.0,\n       5209310.0, 5209290.0, 5209270.0, 5209250.0],\n      dtype='float64', name='y', length=212))xPandasIndexPandasIndex(Index([656330.0, 656350.0, 656370.0, 656390.0, 656410.0, 656430.0, 656450.0,\n       656470.0, 656490.0, 656510.0,\n       ...\n       660830.0, 660850.0, 660870.0, 660890.0, 660910.0, 660930.0, 660950.0,\n       660970.0, 660990.0, 661010.0],\n      dtype='float64', name='x', length=235))Attributes: (1)AREA_OR_POINT :Area\n\n\n\nsnow_cover_downsampled.plot()\n\n<matplotlib.collections.QuadMesh at 0x73dbcc5e86e0>"
  },
  {
    "objectID": "code-snippets/xarray/00_rf_predict_map_blocks.html",
    "href": "code-snippets/xarray/00_rf_predict_map_blocks.html",
    "title": "ds.map_blocks(…)",
    "section": "",
    "text": ".map_blocks(...) applies a function to chunks of a dask-backed xarray.Dataset. The following example demonstrates how ds.map_blocks(...) can be used for pixel-wise application of a machine learning model."
  },
  {
    "objectID": "code-snippets/xarray/00_rf_predict_map_blocks.html#generate-new-data-to-predict-on",
    "href": "code-snippets/xarray/00_rf_predict_map_blocks.html#generate-new-data-to-predict-on",
    "title": "ds.map_blocks(…)",
    "section": "Generate new data to predict on",
    "text": "Generate new data to predict on\nThe time dimension in the following example is only a placeholder for any kind of predictor dimension. For the example to make sense (and work!), the predictor/feature (i.e., time) dimension must not be chunked internally, i.e., form a single chunk!\n\n\nCode\nn_classes = 2\nn_features = 12\nn_samples = 1000\n\nlat = 4000\nlon = 6000 \ntime = n_features\n\n\n\n# random training data\nX_train, y_train = generate_X_y(n_samples,n_features, n_classes)\n\n\n# random features to predict on, in a \"real\" shape (x, y, time)\nds = generate_3d_dataset(lat, lon, time)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset> Size: 2GB\nDimensions:  (lat: 4000, lon: 6000, time: 12)\nCoordinates:\n  * lat      (lat) int64 32kB 0 1 2 3 4 5 6 ... 3994 3995 3996 3997 3998 3999\n  * lon      (lon) int64 48kB 0 1 2 3 4 5 6 ... 5994 5995 5996 5997 5998 5999\n  * time     (time) datetime64[ns] 96B 2021-01-01 2021-01-02 ... 2021-01-12\nData variables:\n    test     (lat, lon, time) float64 2GB dask.array<chunksize=(400, 600, 12), meta=np.ndarray>xarray.DatasetDimensions:lat: 4000lon: 6000time: 12Coordinates: (3)lat(lat)int640 1 2 3 4 ... 3996 3997 3998 3999array([   0,    1,    2, ..., 3997, 3998, 3999], shape=(4000,))lon(lon)int640 1 2 3 4 ... 5996 5997 5998 5999array([   0,    1,    2, ..., 5997, 5998, 5999], shape=(6000,))time(time)datetime64[ns]2021-01-01 ... 2021-01-12array(['2021-01-01T00:00:00.000000000', '2021-01-02T00:00:00.000000000',\n       '2021-01-03T00:00:00.000000000', '2021-01-04T00:00:00.000000000',\n       '2021-01-05T00:00:00.000000000', '2021-01-06T00:00:00.000000000',\n       '2021-01-07T00:00:00.000000000', '2021-01-08T00:00:00.000000000',\n       '2021-01-09T00:00:00.000000000', '2021-01-10T00:00:00.000000000',\n       '2021-01-11T00:00:00.000000000', '2021-01-12T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)test(lat, lon, time)float64dask.array<chunksize=(400, 600, 12), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         2.15 GiB \n                         21.97 MiB \n                    \n                    \n                    \n                         Shape \n                         (4000, 6000, 12) \n                         (400, 600, 12) \n                    \n                    \n                         Dask graph \n                         100 chunks in 1 graph layer \n                    \n                    \n                         Data type \n                         float64 numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  12\n  6000\n  4000\n\n        \n    \nIndexes: (3)latPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999],\n      dtype='int64', name='lat', length=4000))lonPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999],\n      dtype='int64', name='lon', length=6000))timePandasIndexPandasIndex(DatetimeIndex(['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04',\n               '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08',\n               '2021-01-09', '2021-01-10', '2021-01-11', '2021-01-12'],\n              dtype='datetime64[ns]', name='time', freq='D'))Attributes: (0)"
  },
  {
    "objectID": "code-snippets/xarray/00_rf_predict_map_blocks.html#train-a-dummy-model",
    "href": "code-snippets/xarray/00_rf_predict_map_blocks.html#train-a-dummy-model",
    "title": "ds.map_blocks(…)",
    "section": "Train a dummy model",
    "text": "Train a dummy model\n\nrf = RF(random_state=42, n_estimators=50, n_jobs=-1)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)"
  },
  {
    "objectID": "code-snippets/xarray/00_rf_predict_map_blocks.html#function-for-chunk-wise-application",
    "href": "code-snippets/xarray/00_rf_predict_map_blocks.html#function-for-chunk-wise-application",
    "title": "ds.map_blocks(…)",
    "section": "Function for chunk-wise application",
    "text": "Function for chunk-wise application\n\ndef generic_func(ds: xr.Dataset):\n    \"\"\"\n    Flatten chunk\n    Apply Random Forest model\n    Recover original 2D shape\n    \"\"\"\n    ds_stacked = ds.stack(ml=(\"lat\", \"lon\")).transpose(\"ml\", \"time\")\n\n    # predict on input data\n    X = ds_stacked.test.data\n    y_hat_1d = rf.predict(X)\n    y_hat_2d = y_hat_1d.reshape((ds.lat.size, ds.lon.size))\n\n    # copy the chunk but remove (squeeze) the time dimension\n    data_out = ds.isel(time=[0]).squeeze().copy(deep=True)\n    data_out.test.data = y_hat_2d\n\n    return data_out\n\n\nds_pred = ds.map_blocks(generic_func, template=ds.isel(time=[0]).squeeze())\n\n\nds_pred\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset> Size: 192MB\nDimensions:  (lat: 4000, lon: 6000)\nCoordinates:\n  * lat      (lat) int64 32kB 0 1 2 3 4 5 6 ... 3994 3995 3996 3997 3998 3999\n  * lon      (lon) int64 48kB 0 1 2 3 4 5 6 ... 5994 5995 5996 5997 5998 5999\n    time     datetime64[ns] 8B dask.array<chunksize=(), meta=np.ndarray>\nData variables:\n    test     (lat, lon) float64 192MB dask.array<chunksize=(400, 600), meta=np.ndarray>xarray.DatasetDimensions:lat: 4000lon: 6000Coordinates: (3)lat(lat)int640 1 2 3 4 ... 3996 3997 3998 3999array([   0,    1,    2, ..., 3997, 3998, 3999], shape=(4000,))lon(lon)int640 1 2 3 4 ... 5996 5997 5998 5999array([   0,    1,    2, ..., 5997, 5998, 5999], shape=(6000,))time()datetime64[ns]dask.array<chunksize=(), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         8 B \n                         8 B \n                    \n                    \n                    \n                         Shape \n                         () \n                         () \n                    \n                    \n                         Dask graph \n                         1 chunks in 4 graph layers \n                    \n                    \n                         Data type \n                         datetime64[ns] numpy.ndarray \n                    \n                \n            \n        \n        \n        \n        \n    \nData variables: (1)test(lat, lon)float64dask.array<chunksize=(400, 600), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         183.11 MiB \n                         1.83 MiB \n                    \n                    \n                    \n                         Shape \n                         (4000, 6000) \n                         (400, 600) \n                    \n                    \n                         Dask graph \n                         100 chunks in 4 graph layers \n                    \n                    \n                         Data type \n                         float64 numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  6000\n  4000\n\n        \n    \nIndexes: (2)latPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999],\n      dtype='int64', name='lat', length=4000))lonPandasIndexPandasIndex(Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n       ...\n       5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999],\n      dtype='int64', name='lon', length=6000))Attributes: (0)\n\n\n\nds_pred = ds_pred.compute()"
  },
  {
    "objectID": "code-snippets/xarray/02_resample_custom_func.html",
    "href": "code-snippets/xarray/02_resample_custom_func.html",
    "title": "Temporal resampling/aggregation",
    "section": "",
    "text": "import xarray as xr\nimport numpy as np\nimport pandas as pd\n\nfrom util import generate_3d_dataset"
  },
  {
    "objectID": "code-snippets/xarray/02_resample_custom_func.html#some-random-data",
    "href": "code-snippets/xarray/02_resample_custom_func.html#some-random-data",
    "title": "Temporal resampling/aggregation",
    "section": "Some random data",
    "text": "Some random data\n\nlat, lon, time = 40, 60, 120\nds = generate_3d_dataset(lat, lon, time)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<xarray.Dataset> Size: 2MB\nDimensions:  (lat: 40, lon: 60, time: 120)\nCoordinates:\n  * lat      (lat) int64 320B 0 1 2 3 4 5 6 7 8 9 ... 31 32 33 34 35 36 37 38 39\n  * lon      (lon) int64 480B 0 1 2 3 4 5 6 7 8 9 ... 51 52 53 54 55 56 57 58 59\n  * time     (time) datetime64[ns] 960B 2021-01-01 2021-01-02 ... 2021-04-30\nData variables:\n    test     (lat, lon, time) float64 2MB dask.array<chunksize=(4, 6, 120), meta=np.ndarray>xarray.DatasetDimensions:lat: 40lon: 60time: 120Coordinates: (3)lat(lat)int640 1 2 3 4 5 6 ... 34 35 36 37 38 39array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39])lon(lon)int640 1 2 3 4 5 6 ... 54 55 56 57 58 59array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59])time(time)datetime64[ns]2021-01-01 ... 2021-04-30array(['2021-01-01T00:00:00.000000000', '2021-01-02T00:00:00.000000000',\n       '2021-01-03T00:00:00.000000000', '2021-01-04T00:00:00.000000000',\n       '2021-01-05T00:00:00.000000000', '2021-01-06T00:00:00.000000000',\n       '2021-01-07T00:00:00.000000000', '2021-01-08T00:00:00.000000000',\n       '2021-01-09T00:00:00.000000000', '2021-01-10T00:00:00.000000000',\n       '2021-01-11T00:00:00.000000000', '2021-01-12T00:00:00.000000000',\n       '2021-01-13T00:00:00.000000000', '2021-01-14T00:00:00.000000000',\n       '2021-01-15T00:00:00.000000000', '2021-01-16T00:00:00.000000000',\n       '2021-01-17T00:00:00.000000000', '2021-01-18T00:00:00.000000000',\n       '2021-01-19T00:00:00.000000000', '2021-01-20T00:00:00.000000000',\n       '2021-01-21T00:00:00.000000000', '2021-01-22T00:00:00.000000000',\n       '2021-01-23T00:00:00.000000000', '2021-01-24T00:00:00.000000000',\n       '2021-01-25T00:00:00.000000000', '2021-01-26T00:00:00.000000000',\n       '2021-01-27T00:00:00.000000000', '2021-01-28T00:00:00.000000000',\n       '2021-01-29T00:00:00.000000000', '2021-01-30T00:00:00.000000000',\n       '2021-01-31T00:00:00.000000000', '2021-02-01T00:00:00.000000000',\n       '2021-02-02T00:00:00.000000000', '2021-02-03T00:00:00.000000000',\n       '2021-02-04T00:00:00.000000000', '2021-02-05T00:00:00.000000000',\n       '2021-02-06T00:00:00.000000000', '2021-02-07T00:00:00.000000000',\n       '2021-02-08T00:00:00.000000000', '2021-02-09T00:00:00.000000000',\n       '2021-02-10T00:00:00.000000000', '2021-02-11T00:00:00.000000000',\n       '2021-02-12T00:00:00.000000000', '2021-02-13T00:00:00.000000000',\n       '2021-02-14T00:00:00.000000000', '2021-02-15T00:00:00.000000000',\n       '2021-02-16T00:00:00.000000000', '2021-02-17T00:00:00.000000000',\n       '2021-02-18T00:00:00.000000000', '2021-02-19T00:00:00.000000000',\n       '2021-02-20T00:00:00.000000000', '2021-02-21T00:00:00.000000000',\n       '2021-02-22T00:00:00.000000000', '2021-02-23T00:00:00.000000000',\n       '2021-02-24T00:00:00.000000000', '2021-02-25T00:00:00.000000000',\n       '2021-02-26T00:00:00.000000000', '2021-02-27T00:00:00.000000000',\n       '2021-02-28T00:00:00.000000000', '2021-03-01T00:00:00.000000000',\n       '2021-03-02T00:00:00.000000000', '2021-03-03T00:00:00.000000000',\n       '2021-03-04T00:00:00.000000000', '2021-03-05T00:00:00.000000000',\n       '2021-03-06T00:00:00.000000000', '2021-03-07T00:00:00.000000000',\n       '2021-03-08T00:00:00.000000000', '2021-03-09T00:00:00.000000000',\n       '2021-03-10T00:00:00.000000000', '2021-03-11T00:00:00.000000000',\n       '2021-03-12T00:00:00.000000000', '2021-03-13T00:00:00.000000000',\n       '2021-03-14T00:00:00.000000000', '2021-03-15T00:00:00.000000000',\n       '2021-03-16T00:00:00.000000000', '2021-03-17T00:00:00.000000000',\n       '2021-03-18T00:00:00.000000000', '2021-03-19T00:00:00.000000000',\n       '2021-03-20T00:00:00.000000000', '2021-03-21T00:00:00.000000000',\n       '2021-03-22T00:00:00.000000000', '2021-03-23T00:00:00.000000000',\n       '2021-03-24T00:00:00.000000000', '2021-03-25T00:00:00.000000000',\n       '2021-03-26T00:00:00.000000000', '2021-03-27T00:00:00.000000000',\n       '2021-03-28T00:00:00.000000000', '2021-03-29T00:00:00.000000000',\n       '2021-03-30T00:00:00.000000000', '2021-03-31T00:00:00.000000000',\n       '2021-04-01T00:00:00.000000000', '2021-04-02T00:00:00.000000000',\n       '2021-04-03T00:00:00.000000000', '2021-04-04T00:00:00.000000000',\n       '2021-04-05T00:00:00.000000000', '2021-04-06T00:00:00.000000000',\n       '2021-04-07T00:00:00.000000000', '2021-04-08T00:00:00.000000000',\n       '2021-04-09T00:00:00.000000000', '2021-04-10T00:00:00.000000000',\n       '2021-04-11T00:00:00.000000000', '2021-04-12T00:00:00.000000000',\n       '2021-04-13T00:00:00.000000000', '2021-04-14T00:00:00.000000000',\n       '2021-04-15T00:00:00.000000000', '2021-04-16T00:00:00.000000000',\n       '2021-04-17T00:00:00.000000000', '2021-04-18T00:00:00.000000000',\n       '2021-04-19T00:00:00.000000000', '2021-04-20T00:00:00.000000000',\n       '2021-04-21T00:00:00.000000000', '2021-04-22T00:00:00.000000000',\n       '2021-04-23T00:00:00.000000000', '2021-04-24T00:00:00.000000000',\n       '2021-04-25T00:00:00.000000000', '2021-04-26T00:00:00.000000000',\n       '2021-04-27T00:00:00.000000000', '2021-04-28T00:00:00.000000000',\n       '2021-04-29T00:00:00.000000000', '2021-04-30T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)test(lat, lon, time)float64dask.array<chunksize=(4, 6, 120), meta=np.ndarray>\n    \n        \n            \n                \n                    \n                         \n                         Array \n                         Chunk \n                    \n                \n                \n                    \n                    \n                         Bytes \n                         2.20 MiB \n                         22.50 kiB \n                    \n                    \n                    \n                         Shape \n                         (40, 60, 120) \n                         (4, 6, 120) \n                    \n                    \n                         Dask graph \n                         100 chunks in 1 graph layer \n                    \n                    \n                         Data type \n                         float64 numpy.ndarray \n                    \n                \n            \n        \n        \n        \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  120\n  60\n  40\n\n        \n    \nIndexes: (3)latPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39],\n      dtype='int64', name='lat'))lonPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59],\n      dtype='int64', name='lon'))timePandasIndexPandasIndex(DatetimeIndex(['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04',\n               '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08',\n               '2021-01-09', '2021-01-10',\n               ...\n               '2021-04-21', '2021-04-22', '2021-04-23', '2021-04-24',\n               '2021-04-25', '2021-04-26', '2021-04-27', '2021-04-28',\n               '2021-04-29', '2021-04-30'],\n              dtype='datetime64[ns]', name='time', length=120, freq='D'))Attributes: (0)"
  },
  {
    "objectID": "code-snippets/xarray/02_resample_custom_func.html#resample-to-monthly-interval",
    "href": "code-snippets/xarray/02_resample_custom_func.html#resample-to-monthly-interval",
    "title": "Temporal resampling/aggregation",
    "section": "Resample to monthly interval",
    "text": "Resample to monthly interval\n\nda_monthly_mean = ds.test.resample(time=\"1MS\").mean()\n\n\ndef custom_agg_func(da: xr.DataArray) -> xr.DataArray:\n    # dummy operation - could by anything\n    return da.sum(dim=\"time\") / da.count(dim=\"time\")\n\n\nda_monthly_mean_custom = ds.test.resample(time=\"1MS\").apply(custom_agg_func)\n\n\nassert (da_monthly_mean_custom == da_monthly_mean).all()"
  },
  {
    "objectID": "code-snippets/xarray/util.html",
    "href": "code-snippets/xarray/util.html",
    "title": "util.py",
    "section": "",
    "text": "Just a helper to generate some data for training and inference.\nimport pandas as pd\nimport dask.array as da\nimport xarray as xr\n\n\ndef generate_X_y(\n    samples=1000, features=10, classes=2\n) -> tuple[da.core.Array, da.core.Array]:\n    \"\"\"Generate random features and labels\n\n    Parameters\n    ----------\n    samples : int, optional\n        Number of samples, by default 1000\n    features : int, optional\n        Number of features, by default 10\n    classes : int, optional\n        Number of classes, by default 2\n\n    Returns\n    -------\n    tuple[da.core.Array, da.core.Array]\n        X and y, with shape (samples, features) and (samples,)\n    \"\"\"\n    X = da.random.random((samples, features))\n    y = da.random.randint(0, classes, samples)\n\n    return X, y\n\n\ndef generate_3d_dataset(lat=40, lon=60, time=12) -> xr.Dataset:\n    \"\"\"Generate a 3-dimensional xarray.Dataset with lat, lon and time dimensions of\n    the specified size. lat/lon chunks will be int(dim.size/10), time will be a single chunk (-1)\n\n    Parameters\n    ----------\n    lat : int, optional\n        Size of lat dimension, by default 40\n    lon : int, optional\n        Size of lon dimension, by default 60\n    time : int, optional\n        Size of time dimension, by default 12\n\n    Returns\n    -------\n    xr.Dataset\n        Dask-backed dataset.\n    \"\"\"\n    lat_chunk, lon_chunk = int(lat / 10), int(lon / 10)\n    lat_coords = da.arange(lat)\n    lon_coords = da.arange(lon)\n    start = pd.Timestamp(year=2021, month=1, day=1)\n    time_coords = pd.date_range(start, start + pd.Timedelta(days=time - 1), freq=\"1D\")\n    data = da.random.random(\n        (lat_coords.size, lon_coords.size, time_coords.size),\n        chunks=(lat_chunk, lon_chunk, -1),\n    )\n\n    ds = xr.DataArray(\n        data,\n        coords=[lat_coords, lon_coords, time_coords],\n        dims=[\"lat\", \"lon\", \"time\"],\n        name=\"test\",\n    ).to_dataset()\n\n    return ds"
  },
  {
    "objectID": "posts/dem-tirol/index.html",
    "href": "posts/dem-tirol/index.html",
    "title": "High-resolution elevation data for Tyrol",
    "section": "",
    "text": "Package to download elevation data for the State of Tyrol (Austria) with a spatial resolution of 50 cm. This dataset and all the lower resolution products all are available for free under the CC BY 4.0 license here. There is also a WCS for the 50 cm dataset, which, depending on the goal, might be easier to use than this package.\n Link to Gitlab Repo\n\nimport sys\nfrom pathlib import Path\nimport tiroldem as tdem\n\n\nprint(sys.version)\nprint(tdem.__version__)\n\n3.13.1 | packaged by conda-forge | (main, Dec  5 2024, 21:23:54) [GCC 13.3.0]\n0.1.0\n\n\n\nDefine area of interest\n\ncrs = 25832\n\np1 = (654360.437,5214275.175)\np2 = (654360.437,5208326.626)\np3 = (664226.125,5208326.626)\np4 = (664226.125,5214275.175)\n\nshape = [p1, p2, p3, p4]\n\n\n\nDownload individual tiles\n\ntiles_per_region = tdem.get_intersecting_tiles(shape, crs=crs)\n\n# tiles are returned by region\nregion = \"gk-west\"\ntiles = tiles_per_region[region]\n\n# specify output directory\nout_dir = Path(\"...\")\n\ntdem.download_tiles(tiles, out_dir, region)\n\n\n\nMerging individual tiles after downloading\nVarious ways exist to merge individual tiles, which is the reason this is not covered by the package. A simple way is to use gdal_merge.py included in the gdal command line utilities (which likely are already available if the gdal package is installed in the current python environment): gdal_merge -o dem_merged.tif -n <no_data_value_in> -a_nodata <no_data_value_out> dgm*.tif"
  },
  {
    "objectID": "posts/lwd-tirol-download/index.html",
    "href": "posts/lwd-tirol-download/index.html",
    "title": "LWD Tirol Meteo Download Package",
    "section": "",
    "text": "The package lwdmeteo is meant to facilitate download of meteorological data from the Avalanche Warning Service of Tyrol (Lawinenwarndienst Tirol, LWD Tirol). The package allows to retrieve a file (via get_station_file()) that contains all stations with some basic metadata which allows to explore when measurements start and what the corresponding station identifier (lwd-nummer) is. This station id must be passed to the actual download function (download_station(...) or batch_download_station(...)). A command line interface is also available with a description in the repository.\nOn the side of LWD Tirol, the files can be downloaded here for a specific station, variable and hydrological year. The lwdmeteo package simply sends requests to very same url that is accessed when downloading files via this page. Note that the data is licensed under CC BY 4.0.\n\n\n\n\n\n\nWarning\n\n\n\nThe data available for download seems to be “raw” data and therefore should be carefully evaluated.\n\n\nFor the purpose of visualization, in the following I will use Sep. 1 as the start of the “hydrological” year, as I think it is a bit nicer since it captures snowfall events early in the season. Therefore I “define” the hydrological year in the following starting Sep. 1 which does not adhere to the actual definition.\n Link to Gitlab Repo\n\nimport sys\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport contextily as cx\nimport lwdmeteo\nfrom lwdmeteo import batch_download_station, get_station_file, parameters\n\nprint(sys.version)\nprint(pd.__version__)\nprint(np.__version__)\nprint(sns.__version__)\nprint(mpl.__version__)\nprint(cx.__version__)\nprint(lwdmeteo.__version__)\n\n3.13.1 | packaged by conda-forge | (main, Dec  5 2024, 21:23:54) [GCC 13.3.0]\n2.2.3\n2.2.0\n0.13.2\n3.10.1\n1.6.2\n0.1.0\n\n\n\nExploring available parameters\nThe keys of the dictionary are used to specify the desired parameter in the download function.\n\nparameters\n\n{'LT': 'Lufttemperatur',\n 'LF': 'Luftfeuchte',\n 'WG': 'Windgeschwindigkeit',\n 'WR': 'Windrichtung',\n 'GS': 'Globalstrahlung von oben',\n 'HS': 'Schneehoehe',\n 'T0': 'Oberflächentemperatur',\n 'TP': 'Taupunkttemperatur',\n 'WG.Boe': 'Windböe',\n 'GS.unten': 'reflektierte Globalstrahlung'}\n\n\n\n\nExploring available stations\n\ngdf_stations = get_station_file()\ngdf_stations.head()\n\n\n\n\n\n  \n    \n      \n      name\n      operator\n      lwd-nummer\n      begin\n      altitude\n      geometry\n    \n  \n  \n    \n      0\n      Teufelsegg - Giovo del Diavolo\n      Südtirol - Alto Adige\n      STEU2\n      2007\n      3035.0\n      POINT (10.76466 46.7847)\n    \n    \n      1\n      Schaufeljoch\n      LWD Tirol\n      NSGS1\n      2012\n      3160.0\n      POINT (11.1109 46.9772)\n    \n    \n      2\n      Rossbänke - Pian dei Cavalli\n      Südtirol - Alto Adige\n      WROS2\n      2001\n      2255.0\n      POINT (10.81944 46.46935)\n    \n    \n      3\n      Lämmerbichlalm\n      LWD Tirol\n      TLAE2\n      1994\n      2020.0\n      POINT (11.75172 47.18127)\n    \n    \n      4\n      Niederthai\n      Tiroler Wasserkraft AG\n      UNIE2\n      1975\n      1564.0\n      POINT (10.96907 47.12284)\n    \n  \n\n\n\n\n\n\nVisualization of station locations\nNote that get_station_file() parses the response from here. This means that the following plot could look different if there are any updates in the future.\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\ngdf_stations.plot(ax=ax, color=\"r\", markersize=10)\ncx.add_basemap(ax, crs=gdf_stations.crs)\n\nplt.show()\n\n\n\n\n\n\nSelect station and retrieve measurement start\n\nstation = \"LPUI2\"  # Puitegg\n\ngdf = get_station_file()\ngdf.loc[gdf[\"lwd-nummer\"] == station]\n\n\n\n\n\n  \n    \n      \n      name\n      operator\n      lwd-nummer\n      begin\n      altitude\n      geometry\n    \n  \n  \n    \n      203\n      Puitegg\n      LWD Tirol\n      LPUI2\n      2002\n      1539.0\n      POINT (11.15282 47.39484)\n    \n  \n\n\n\n\n\n\nLet’s download the entire timeseries\n\nparameter = \"HS\"  # snow depth\n\nstart_year = 2002\nend_year = 2025\n\ndf = batch_download_station(station, parameter, start_year, end_year)\n\n# resample to daily mean\ndf = df.resample(\"1D\").mean()\n\n\n# add hydrological year to df\ndf[\"hydro_year\"] = df.index.to_series().dt.year + (df.index.to_series().dt.month >= 9)\n\n\n\nSome (maybe overly complicated?) method to add the day-of-year starting september first\n\ndf[\"hydro_doy\"] = df.index.to_series().dt.day_of_year\ndoy_values = np.copy(df.index.to_series().dt.day_of_year.values)\n\nbreak_value = (\n    244 + df.index.to_series().dt.is_leap_year\n)  # DOY of Sep. 1 minus 1 day; in leap years Sep. 1\nminus = break_value - 1\nplus = 122  # nr of days after Sep. 1 - same every year\n\ndf.loc[doy_values >= break_value, \"hydro_doy\"] = (\n    df.loc[doy_values >= break_value, \"hydro_doy\"] - minus\n)\ndf.loc[doy_values < break_value, \"hydro_doy\"] = (\n    df.loc[doy_values < break_value, \"hydro_doy\"] + plus\n)\n\n\n\nClean up\n\nSet values above 1 m snow depth in July and August to nan, as I assume these are measurement errors\nRemove 2013, 2014 and 2015, it seems the data contains erroneous values in these years (not shown here)\n\n\ncond = (\n    (df.index.to_series().dt.month < 9)\n    & (df.index.to_series().dt.month > 6)\n    & (df[\"HS\"] > 100)\n)\ndf.loc[cond, \"HS\"] = np.nan\n\ndf = df.loc[~df[\"hydro_year\"].isin([2013, 2014, 2015])]\n\nCompute percentiles to be added to the plots as additional info (can be easily exchanged for min/max or other metrics)\n\n\nCode\nmin_max = (\n    df.groupby(\"hydro_doy\")[[\"HS\"]]\n    .agg(\n        min=(\"HS\", lambda x: np.nanpercentile(x, 5)),\n        max=(\"HS\", lambda x: np.nanpercentile(x, 95)),\n        med=(\"HS\", lambda x: np.nanmedian(x)),\n    )\n    .reset_index()\n)\n\n\nAdd upper and lower bounds of entire timeseries to original data frame for easier access\n\ndf_merged = df.reset_index().merge(min_max, on=\"hydro_doy\").set_index(\"timestamp\")\ndf_merged.head()\n\n\n\n\n\n  \n    \n      \n      HS\n      hydro_year\n      hydro_doy\n      min\n      max\n      med\n    \n    \n      timestamp\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2002-10-03\n      0.000000\n      2003\n      33\n      -3.581250\n      19.069733\n      6.093536\n    \n    \n      2002-10-04\n      0.194444\n      2003\n      34\n      -2.737500\n      18.760327\n      5.895833\n    \n    \n      2002-10-05\n      0.305556\n      2003\n      35\n      -1.600694\n      18.482539\n      7.243056\n    \n    \n      2002-10-06\n      0.805556\n      2003\n      36\n      -2.391667\n      20.547516\n      8.152778\n    \n    \n      2002-10-07\n      0.847222\n      2003\n      37\n      -3.656250\n      21.091667\n      9.930556\n    \n  \n\n\n\n\n\n\nVisualize\nTitles specify the “hydrological year” starting Sep. 1.\n\n\nCode\n# generate ticklabels\ndates_x = pd.date_range(start=\"2021-09-01\", end=\"2022-08-31\", freq=\"2MS\")\nticklabels = dates_x.to_series().dt.strftime(\"%B\").values\ntickpos = dates_x.to_series().dt.day_of_year.values\ndayvals = tickpos.copy()\n\ntickpos[dayvals >= 244] = tickpos[dayvals >= 244] - 243\ntickpos[dayvals < 244] = tickpos[dayvals < 244] + 122\n\nsns.set_theme(font_scale=1.5)\n\ng = sns.FacetGrid(\n    df_merged,\n    col=\"hydro_year\",\n    col_wrap=4,\n    sharey=True,\n    sharex=True,\n    height=5,\n    aspect=1.2,\n)\n\n\ndef plot_with_minmax(data, color, **kwargs):\n    plt.fill_between(\n        data[\"hydro_doy\"],\n        data[\"min\"],\n        data[\"max\"],\n        color=\"lightgrey\",\n        label=\"5 % and 95 % quantile\",\n        alpha=0.5,\n    )\n\n    # use matplotlib directly, to avoid plotting NaN timesteps\n    plt.plot(data[\"hydro_doy\"], data[\"HS\"], linewidth=2, label=\"This year's data\")\n    \n    sns.lineplot(\n        data=data, x=\"hydro_doy\", y=\"med\", linewidth=1, label=\"Long-term median\"\n    )\n    plt.title(data[\"hydro_year\"].iloc[0])\n\n\ng.map_dataframe(plot_with_minmax)\n\n\ndef set_lim_label(color, xmin, xmax, ymax):\n    ax = plt.gca()\n\n    ax.set_xticks(tickpos, ticklabels, rotation=45, ha=\"right\")\n\n    ax.set_xlim((xmin, xmax))\n    ax.set_ylim((0, ymax))\n    ax.set_ylabel(\"Snow depth [cm]\")\n    ax.set_xlabel(None)\n\n\ng.map(set_lim_label, xmin=1, xmax=366, ymax=300)\n\ng.add_legend(loc=\"lower left\", bbox_to_anchor=(0.1, 1.01), ncol=3)\n\nplt.savefig(f\"{station}-snow-depth.png\", dpi=100, bbox_inches=\"tight\")\nplt.show()"
  },
  {
    "objectID": "posts/function-dispatch/index.html",
    "href": "posts/function-dispatch/index.html",
    "title": "Function dispatch",
    "section": "",
    "text": "Sometimes I want to select a function from a set of functions based on a string. The examples here are compiled from this, this and this stackoverflow post.\n\n\nProbably the easiest, most straightforward method.\ndef func1():\n    print(\"Running func1\")\n\ndef func2():\n    print(\"Running func2\")\n\ndef func3():\n    print(\"Running func3\")\n\nfuncs = {\n    \"func1\": func1,\n    \"func2\": func2,\n    \"func3\": func3,\n}\n\ndef top_level_func(function=\"func1\"):\n    f = funcs[function]\n    f()\n\n\n\nNice and clean due to import from external module (although that could easily be achieved for option 1, too)\nmod.py\ndef func1():\n    print(\"Running func1\")\n\ndef func2():\n    print(\"Running func2\")\n\ndef func3():\n    print(\"Running func3\")\nimport mod\n\ndef top_level_func(function=\"func1\"):\n    f = getattr(mod, function)\n    f()\n\n\n\nAdapted from this post on stackoverflow.\nclass FunctionContainer:\n    def __init__(self):\n        self.funcs = {}\n\n    def _add(self, f):\n        self.funcs[f.__name__] = f\n\n    def add(self, f):\n        self._add(f)\n\n        return f\n\n    def __getitem__(self, function_name):\n        return self.funcs[function_name]\n\n\ncontainer = FunctionContainer()\n\n@container.add\ndef func1():\n    print(\"Running func1\")\n\n@container.add\ndef func2():\n    print(\"Running func2\")\n\n@container.add\ndef func3():\n    print(\"Running func3\")\n\ndef top_level_func(function=\"func1\"):\n    f = container[function]\n    f()\n\n\n\nPeople also suggest to use eval or globals() for this case. Both are not ideal, due to the execution of arbitrary code or access to global variables in some way, which likely can be avoided in most cases."
  },
  {
    "objectID": "posts/python-tools/index.html",
    "href": "posts/python-tools/index.html",
    "title": "Some tools to write better python code",
    "section": "",
    "text": "For some time now python supports type hints. These allow to add type annotations to variables and function declarations, much like they are required in many statically typed languages (C, C++, …). They are called “annotations” or “hints” because they are not enforced (i.e., checked) at runtime, although tools like mypy and pyright can be used to perform type checks.\nAlthough type hints should not be used to substitute function docstrings, they help a lot to document code and improve readability and could be ultimately used to perform type checking."
  },
  {
    "objectID": "posts/python-tools/index.html#visual-studio-code-extensions",
    "href": "posts/python-tools/index.html#visual-studio-code-extensions",
    "title": "Some tools to write better python code",
    "section": "Visual Studio Code extensions",
    "text": "Visual Studio Code extensions\n\nPython extension\nBasic extension for writing python code in VS Code. Integrates the python debugger, allows to specify and change the interpreter (i.e., environment) and has support for jupyter notebooks among many other things.\n\n\nAutomatically format code (black and ruff)\nThis is probably the easiest and cheapest way to improve readability and help to adhere to some of pythons coding standards (or PEPs). If you work on your own codebase, I think there are few reasons not to (auto-)format your code. Setting this up is straight forward and takes at most a few minutes (if you include the time for reading up on different formatters).\n\nSteps\n\nFrom this list, choose and install a code formatter (I previously used black, but ruff offers the same functionality and is a lot faster)\nConfigure your default formatter.\nSet VSCode to automatically format code when saving a file\n\n\n\n\nautoDocstring\nGenerate docstrings for functions and methods. After placing the cursor below the function or method header, hitting Ctrl + Shift + 2 will generate a correctly formatted docstring template with the names of arguments where you only need to add a little bit of description. In the settings you can choose between different docstring formats. I personally prefer the numpy style.\nIf you use type hints, autoDocstring will pick these up from the function header and add them to the docstring template, which is quite convenient."
  },
  {
    "objectID": "posts/geosphere-download/index.html",
    "href": "posts/geosphere-download/index.html",
    "title": "Geosphere Data Download",
    "section": "",
    "text": "Link to Gitlab Repo"
  },
  {
    "objectID": "posts/geosphere-download/index.html#description",
    "href": "posts/geosphere-download/index.html#description",
    "title": "Geosphere Data Download",
    "section": "Description",
    "text": "Description\nThis package is meant to facilitate the download of meteorological data from the Geosphere (formerly ZAMG) Austria data hub. The data is made freely available under the CC BY 4.0 license.\n\n\n\n\n\n\nNote\n\n\n\nThe package relies on asyncio and aiohttp for querying the data and currently can only run successfully in regular python scripts, i.e., it does unfortunately not work when used in jupyter notebooks."
  },
  {
    "objectID": "posts/geosphere-download/index.html#downloading-data",
    "href": "posts/geosphere-download/index.html#downloading-data",
    "title": "Geosphere Data Download",
    "section": "Downloading data",
    "text": "Downloading data\nHere is an example on how to download data for the station in Obergurgl. Batch download funtionality is described here):\nfrom datetime import datetime\nfrom loguru import logger\n\nfrom geospheredl import GeosphereClient\n\ngclient = GeosphereClient()\n\n# pick a dataset\nds = \"/station/historical/klima-v2-1h\"\n\n# set a dataset\ngclient.set_dataset(ds)\n\nvariables = [\"tl\", \"cglo\", \"ff\", \"rr\", \"rf\", \"sh\"]\nparams = {\n    \"parameters\": variables,\n    \"start_date\": datetime(1995, 1, 1),\n    \"end_date\": datetime(2025, 1, 1),\n    \"station_id\": 66,\n}\n    \ndf = gclient.download_station(**params)\ndf.to_csv(\"obergurgl.csv\")"
  },
  {
    "objectID": "posts/geosphere-download/index.html#visualization",
    "href": "posts/geosphere-download/index.html#visualization",
    "title": "Geosphere Data Download",
    "section": "Visualization",
    "text": "Visualization\nHere is a visualization of the annual temperature distribution based on the downloaded data. The visualization is a slightly adapted version of the seaborn ridgeplot example.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"obergurgl.csv\")\ndf[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\ndf = df[[\"timestamp\", \"tl\"]]\ndf = df.dropna(axis=\"rows\")\ndf[\"year\"] = df[\"timestamp\"].dt.year\ndf[\"doy\"] = df[\"timestamp\"].dt.day_of_year\ndf = df.set_index(\"timestamp\")\ndf = df.loc[df.index < \"2025-01-01\"]\n\nvariable = \"tl\"\n\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\npal = sns.cubehelix_palette(len(df[\"year\"].unique()), rot=-0.25, light=0.7)\ng = sns.FacetGrid(df, row=\"year\", hue=\"year\", aspect=14, height=0.5, palette=pal)\n\nbw = 0.5\n\ng.map(\n    sns.kdeplot, variable, bw_adjust=bw, clip_on=True, fill=True, alpha=1, linewidth=1.5\n)\ng.map(sns.kdeplot, variable, clip_on=True, color=\"w\", lw=2, bw_adjust=bw)\ng.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n\n\ndef label_ax(x, color, label):\n    ax = plt.gca()\n    ax.text(\n        0,\n        0.2,\n        label,\n        fontweight=\"bold\",\n        color=color,\n        ha=\"left\",\n        va=\"center\",\n        transform=ax.transAxes,\n    )\n\n\ng.map(label_ax, \"tl\")\n\n\ndef set_xlim(color, label, xmin, xmax):\n    ax = plt.gca()\n    ax.set_xlim((xmin, xmax))\n\n\ng.map(set_xlim, xmin=-20, xmax=25)\n\ng.figure.subplots_adjust(hspace=-0.25)\n\ng.set_titles(\"\")\ng.set(yticks=[], ylabel=\"\")\ng.despine(bottom=True, left=True)\ng.set_xlabels(\"Air temperature [°C]\")\n\nplt.savefig(\"temp.png\", dpi=100, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\nAnnual air temperature distribution in Obergurgl (Austria) based on 1-hourly measurements\n\n\n\n\n\n\n\n\n\n\nCaveats for downloading a lot of data\n\n\n\nThe geosphere data hub has a limit for the size (i.e., data points) of individual requests and a rate limit for the number of requests. This means it is impossible to download long time series for many different variables in a single request if it exceeds the size limit. Therefore geospheredl tries to detect such cases and splits them temporally in a way such that individual requests do not exceed the size limit. Afterwards, the individual pieces of a timeseries are merged again. If geospheredl hits the rate limit for requests, it sleeps for the time until the rate limit is reset, which is indicated in the response from the data hub. Generally speaking, this means that downloading a lot of data via geospheredl can potentially take a lot of time."
  }
]